{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aleksandrova_ht1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rL6B_B-RldA"
      },
      "source": [
        "Я решила работать с рецензиями на сериал \"Игра престолов\" на кинопоиске"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDYd0EffsGG1",
        "outputId": "af22e999-b25d-4f94-f0fb-3369c99d8904"
      },
      "source": [
        "pip install fake_useragent"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fake_useragent\n",
            "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
            "Building wheels for collected packages: fake-useragent\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=f372bc842b5f14459f9ebe518f5c6c40ba901a5dd4032082b0bca28cac910da4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n",
            "Successfully built fake-useragent\n",
            "Installing collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-0.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Sg5REM7Jta"
      },
      "source": [
        "import requests\n",
        "import nltk\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "from fake_useragent import UserAgent\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwbE4pVMEoRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2007dc13-0bf8-4904-ae2d-7bf112099c86"
      },
      "source": [
        "ua = UserAgent(verify_ssl=False)\n",
        "headers = {'User-Agent': ua.random}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fake_useragent/utils.py\", line 154, in load\n",
            "    for item in get_browsers(verify_ssl=verify_ssl):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fake_useragent/utils.py\", line 99, in get_browsers\n",
            "    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n",
            "IndexError: list index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN4n5qNwtoQA"
      },
      "source": [
        "GOT_pos = 'https://www.kinopoisk.ru/film/464963/reviews/ord/rating/status/good/perpage/50/'\n",
        "GOT_neg = 'https://www.kinopoisk.ru/film/464963/reviews/?status=bad&ord=rating&rnd=1631895190&perpage=50'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek9O_uUrFqUy"
      },
      "source": [
        "def parser(url):\n",
        "  html = BeautifulSoup((requests.get(url, headers=headers)).text)\n",
        "  reviews = html.find_all(\"span\", {\"itemprop\": \"reviewBody\"})\n",
        "  list_reviews = []\n",
        "  for review in reviews:\n",
        "    review = review.get_text()\n",
        "    list_reviews.append(review)\n",
        "  return list_reviews"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g0dpvVIhj39"
      },
      "source": [
        "pos_rev = parser(GOT_pos)\n",
        "neg_rev = parser(GOT_neg)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7KjRwr_ianN"
      },
      "source": [
        "cntrl_pos = pos_rev[:10] #10 контрольных положительных\n",
        "cntrl_neg = neg_rev[:10] #10 контрольных отрицательных\n",
        "pos_revs = pos_rev[10:]\n",
        "neg_revs = neg_rev[10:]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnRfAZGfSRNI"
      },
      "source": [
        "def word_list(some_list):\n",
        "  lo_words = []\n",
        "  for review in some_list:\n",
        "    review_words = nltk.word_tokenize(review.lower())\n",
        "    for word in review_words:\n",
        "      word = lemmatizer.lemmatize(word)\n",
        "      lo_words.append(word)\n",
        "  return lo_words"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv_mBwHSsW_B",
        "outputId": "4ae4052b-42f3-457e-b442-22814db88c16"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HR0TB1BIWlr"
      },
      "source": [
        "poswords = word_list(pos_revs)\n",
        "negwords = word_list(neg_revs)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snn4HL28J1FI"
      },
      "source": [
        "positive_words = [] #список самых попцлярных слов, которые есть только в положительных отзывах\n",
        "for good_word in poswords:\n",
        "  if not good_word in negwords:\n",
        "    positive_words.append(good_word)\n",
        "counts_pos = Counter(positive_words).most_common(30)\n",
        "positive_words = []\n",
        "for word in counts_pos:\n",
        "  positive_words.append(word[0])\n",
        "\n",
        "negative_words = [] #список самых популяных слов, которые есть только в отрицательных отзывах\n",
        "for bad_word in negwords:\n",
        "  if not bad_word in poswords:\n",
        "    negative_words.append(bad_word)\n",
        "counts_neg = Counter(negative_words).most_common(30)\n",
        "negative_words = []\n",
        "for word in counts_neg:\n",
        "  negative_words.append(word[0])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqkAuEdbL7ak"
      },
      "source": [
        "def the_smart_guy(review, pos=positive_words, neg=negative_words):\n",
        "  review_words = []\n",
        "  tokens = nltk.word_tokenize(review)\n",
        "  for token in tokens:\n",
        "    lemma = lemmatizer.lemmatize(token)\n",
        "    review_words.append(lemma)\n",
        "  good_review_words = []\n",
        "  bad_review_words = []\n",
        "  for word in review_words:\n",
        "    if word in pos:\n",
        "      good_review_words.append(word)\n",
        "    if word in neg:\n",
        "      bad_review_words.append(word)\n",
        "  if len(good_review_words) > len(bad_review_words):\n",
        "    answer = 'positive'\n",
        "  elif len(bad_review_words) > len(good_review_words):\n",
        "    answer = 'negative'\n",
        "  else:\n",
        "    answer = 'idk'\n",
        "  print(answer)\n",
        "  return answer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyzFZUdncl8F",
        "outputId": "d8f8bf41-8c26-415e-9cca-d766a70090dc"
      },
      "source": [
        "right_answers = 0\n",
        "\n",
        "for one in cntrl_pos: #проверяем на десяти положительных контрольных отзывах\n",
        "  if the_smart_guy(one) == 'positive':\n",
        "    right_answers += 1\n",
        "print('__')\n",
        "for one in cntrl_neg: #проверяем на десяти отрицательных контрольных отзвах\n",
        "  if the_smart_guy(one) == 'negative':\n",
        "    right_answers += 1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n",
            "positive\n",
            "positive\n",
            "negative\n",
            "positive\n",
            "idk\n",
            "positive\n",
            "positive\n",
            "positive\n",
            "positive\n",
            "__\n",
            "negative\n",
            "negative\n",
            "idk\n",
            "negative\n",
            "negative\n",
            "negative\n",
            "negative\n",
            "idk\n",
            "idk\n",
            "idk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0KP7Bc9d-9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "293fd8b8-704c-409e-a9ee-c2109a1789f7"
      },
      "source": [
        "accuracy = right_answers / (len(cntrl_neg) + len(cntrl_pos)) * 100\n",
        "print('accuracy =', int(accuracy), '%')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 65 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnDkJf18RLAt"
      },
      "source": [
        "Способы улучшить программу:\n",
        "1. на бОльших выборках (кинопоиск очень сопротивлялся вмешательствам) можно попробовать поиграть с частями речи. например, выбрать только прилагательные. На выборке из 80 рецензий такое не очень сработало\n",
        "\n",
        "2. можно воспользоваться каким-нибудь семантическим анализатором как минимум на списке самых часто встречающихся слов из отзывов, потому что даже там часто попадались какие-то весьма странные штуки\n",
        "\n",
        "3. можно как-то посмотреть на количество слов с приставокой/частицей \"не\". интуиция посдказывает, что таких должно быть больше в отрицательных отзывах"
      ]
    }
  ]
}